{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to fetch pets on page 1: 429 - attempt 1\n",
      "‚ùå Failed to fetch pets on page 1: 429 - attempt 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Failed to fetch pets on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempts+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m         attempts += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 5 seconds before retrying\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Giving up on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\"\"\"Petfinder API to pull listings for adoptable pets with pagination and retry logic.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# üîê Step 1: Authenticate and obtain API access token\n",
    "auth_data = {\n",
    "    \"grant_type\": \"client_credentials\",\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET\n",
    "}\n",
    "response = requests.post(TOKEN_URL, data=auth_data)\n",
    "access_token = response.json().get(\"access_token\")\n",
    "\n",
    "if not access_token:\n",
    "    print(f\"‚ùå Failed to obtain access token: {response.json()}\")\n",
    "    exit()\n",
    "\n",
    "# üèó Step 2: Define headers for authentication\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "# üê∂ Step 3: Fetch adoptable pets using pagination with retry logic\n",
    "PET_URL = \"https://api.petfinder.com/v2/animals\"\n",
    "all_pets = []  # Store all pets data\n",
    "\n",
    "page = 1  # Start from first page\n",
    "while True:\n",
    "    params = {\"type\": \"dog\", \"location\": \"75001\", \"limit\": 100, \"page\": page}\n",
    "    success = False\n",
    "    attempts = 0\n",
    "    while not success and attempts < 3:\n",
    "        response = requests.get(PET_URL, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            pet_data = response.json()[\"animals\"]\n",
    "            success = True  # Successful retrieval\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to fetch pets on page {page}: {response.status_code} - attempt {attempts+1}\")\n",
    "            attempts += 1\n",
    "            time.sleep(5)  # Wait 5 seconds before retrying\n",
    "\n",
    "    if not success:\n",
    "        print(f\"‚ùå Giving up on page {page}.\")\n",
    "        break\n",
    "\n",
    "    if not pet_data:  # If no more pets are returned, stop pagination\n",
    "        break\n",
    "\n",
    "    all_pets.extend(pet_data)  # Append current batch of pets\n",
    "    page += 1  # Move to next page\n",
    "\n",
    "# üìä Convert all pet data to a DataFrame\n",
    "df_pets = pd.json_normalize(all_pets)\n",
    "# üéØ Select relevant columns\n",
    "df_pet = df_pets[[\"id\", \"name\", \"breeds.primary\", \"age\", \"organization_id\", \"url\", \"contact.address.city\", \"contact.address.state\"]]\n",
    "df_pet.columns = [\"Pet ID\", \"Name\", \"Breed\", \"Age\", \"Shelter ID\", \"Adoption Link\", \"City\", \"State\"]\n",
    "\n",
    "# üè† Step 4: Fetch shelter details dynamically\n",
    "shelter_ids = df_pet[\"Shelter ID\"].dropna().unique()\n",
    "df_shelters = pd.DataFrame()\n",
    "\n",
    "for shelter_id in shelter_ids:\n",
    "    ORG_URL = f\"https://api.petfinder.com/v2/organizations/{shelter_id}\"\n",
    "    response = requests.get(ORG_URL, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        shelter_data = response.json()[\"organization\"]\n",
    "        df_temp = pd.json_normalize(shelter_data)\n",
    "        df_shelters = pd.concat([df_shelters, df_temp], ignore_index=True)\n",
    "\n",
    "# üéØ Select relevant columns for shelters\n",
    "df_shelters = df_shelters[[\"id\", \"name\", \"address.city\", \"address.state\", \"phone\", \"url\"]]\n",
    "df_shelters.columns = [\"Shelter ID\", \"Shelter Name\", \"City\", \"State\", \"Phone\", \"Website\"]\n",
    "\n",
    "# üîó Merge Pet & Shelter DataFrames\n",
    "df_combined = df_pet.merge(df_shelters, on=\"Shelter ID\", how=\"left\")\n",
    "\n",
    "# üíæ Step 5: Save DataFrames to Excel and CSV\n",
    "df_pet.to_csv(\"adoptable_pets.csv\", index=False)\n",
    "df_shelters.to_csv(\"shelter_details.csv\", index=False)\n",
    "df_combined.to_csv(\"adoptable_pets_shelters.csv\", index=False)\n",
    "df_pet.to_csv(\"original_adoptable_pets.csv\", index=False)\n",
    "df_combined.to_excel(\"adoptable_pets_shelters.xlsx\", index=False, sheet_name=\"Adoption Data\")\n",
    "\n",
    "print(\"‚úÖ Data successfully saved! üéâ\")\n",
    "print(\"üìÇ CSV files: 'adoptable_pets.csv', 'shelter_details.csv', 'adoptable_pets_shelters.csv'\")\n",
    "print(\"üìÇ Excel file: 'adoptable_pets_shelters.xlsx'\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbfdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0359766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8006, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a760fbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pet ID', 'Name', 'Breed', 'Age', 'Shelter ID', 'Adoption Link',\n",
       "       'City_x', 'State_x', 'Shelter Name', 'City_y', 'State_y', 'Phone',\n",
       "       'Website'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88fb9559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pet ID           Name         Breed    Age Shelter ID  \\\n",
      "0  76440074    Little Pepe     Chihuahua  Adult     TX2896   \n",
      "1  76440023    Little Pepe     Chihuahua  Adult     TX2896   \n",
      "2  76440021         Weasel     Dachshund  Young     TX1223   \n",
      "3  76440020        Petunia         Corgi  Young     TX1223   \n",
      "4  76438778  Tori PKA Tory  Aussiedoodle  Adult     TX1568   \n",
      "\n",
      "                                       Adoption Link            City_x  \\\n",
      "0  https://www.petfinder.com/dog/little-pepe-7644...              Krum   \n",
      "1  https://www.petfinder.com/dog/little-pepe-7644...  Highland Village   \n",
      "2  https://www.petfinder.com/dog/weasel-76440021/...         Arlington   \n",
      "3  https://www.petfinder.com/dog/petunia-76440020...         Arlington   \n",
      "4  https://www.petfinder.com/dog/tori-pka-tory-76...            Dallas   \n",
      "\n",
      "  State_x              Shelter Name     City_y State_y           Phone  \\\n",
      "0      TX         Ruff Road Revival       Krum      TX            None   \n",
      "1      TX         Ruff Road Revival       Krum      TX            None   \n",
      "2      TX  Shelter2Rescue Coalition  Arlington      TX  (214) 616-6128   \n",
      "3      TX  Shelter2Rescue Coalition  Arlington      TX  (214) 616-6128   \n",
      "4      TX     Cody's Friends Rescue     Dallas      TX            None   \n",
      "\n",
      "                                             Website  \n",
      "0  https://www.petfinder.com/member/us/tx/krum/ru...  \n",
      "1  https://www.petfinder.com/member/us/tx/krum/ru...  \n",
      "2  https://www.petfinder.com/member/us/tx/arlingt...  \n",
      "3  https://www.petfinder.com/member/us/tx/arlingt...  \n",
      "4  https://www.petfinder.com/member/us/tx/dallas/...  \n"
     ]
    }
   ],
   "source": [
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1040e91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Krum', 'Highland Village', 'Arlington', 'Dallas', 'Carrollton',\n",
       "       'Seven Points', 'Richardson', 'Euless', 'McKinney', 'Fort Worth',\n",
       "       'Sanger', 'Farmers Branch', 'Cedar Hill', 'Granbury', 'Lewisville',\n",
       "       'Quinlan', 'Colleyville', 'Alvarado', 'Plano', 'Gainesville',\n",
       "       'Little Elm', 'Pilot Point', 'Irving', 'Corsicana', 'Denison',\n",
       "       'Weatherford', 'Rockwall', 'Itasca', 'Stephenville', 'Grapevine',\n",
       "       'Forney', 'Waco', 'Ardmore', 'North Richland Hills', 'Frisco',\n",
       "       'Mesquite', 'Justin', 'Graham', 'Keller', 'Grand Prairie',\n",
       "       'Sherman', 'Flower Mound', 'Tyler', 'Greenville', 'Henrietta',\n",
       "       'Farmersville', 'Ennis', 'Southlake', 'Rowlett', 'Point',\n",
       "       'Benbrook', 'Madill', 'Pottsboro', 'Haslet', 'Tishomingo', 'Allen',\n",
       "       'Mineola', 'Waxahachie', 'Garland', 'Sunnyvale', 'Lavon', 'Mexia',\n",
       "       'Joshua', 'Saginaw', 'Commerce', 'Van Alstyne', 'Paris', 'EMORY',\n",
       "       'White Settlement', 'Royse City', 'Springtown', 'Whitewright',\n",
       "       'Mckinney', 'Red Oak', 'fort worth', 'Scroggins', 'Watauga',\n",
       "       'Klondike', 'Saint Jo', 'Wylie', 'Godley', 'Murphy', 'Bowie',\n",
       "       'Mansfield', 'Aledo', 'Cleburne', 'Fort Worth ', 'Cumby',\n",
       "       'ROWLETT', 'GAINESVILLE', 'Mingus', 'Richland Hills',\n",
       "       'Gun Barrel City', 'Celina', 'Denton', 'Ladonia', 'Mabank',\n",
       "       'Cockrell Hill', 'Sachse', 'Dallas ', 'Lake Dallas', 'Cross Roads',\n",
       "       'Seagoville', 'Parker', 'Durant', 'Paradise', 'Meridian',\n",
       "       'Hawkins', 'Pantego', 'Anna', 'Nocona', 'Covington', 'Venus',\n",
       "       'WAXAHACHIE', 'The Colony', 'Argyle', 'River oaks', 'Bonham',\n",
       "       'Weatherford ', 'Ferris', 'Terrell', 'Lone Oak', 'Southlake ',\n",
       "       'Winnsboro', 'Grandview', 'Hillsboro', 'Wills Point', 'Midlothian',\n",
       "       'FRISCO', 'Clifton', 'Azle', 'Princeton', 'Merit', 'Melissa',\n",
       "       'Bridgeport', 'Crowley', 'Alba', 'Frankston', 'ALLEN', 'Everman',\n",
       "       'Haltom City', 'Burleson', 'Coppell', 'Kaufman', 'Boyd'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pets['City'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab9f8004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Krum', 'Arlington', 'Dallas', 'Seven Points', 'Richardson',\n",
       "       'Euless', 'McKinney', 'Warren', 'Fort Worth', 'Mathis',\n",
       "       'Sherman Oaks', 'Farmers Branch', 'Carrollton', 'Cedar Hill',\n",
       "       'Granbury', 'Lewisville', 'Quinlan', 'Colleyville', 'Austin',\n",
       "       'Gainesville', 'Little Elm', 'Pilot Point', 'Irving', 'Corsicana',\n",
       "       'Denison', 'Weatherford', 'Rockwall', 'Itasca', 'Garland',\n",
       "       'Stephenville', 'Conroe', 'Plano', 'Forney', 'Ardmore',\n",
       "       'North Richland Hills', 'Frisco', 'Mesquite', 'Houston',\n",
       "       'Brookeville', 'Justin', 'Graham', 'Keller', 'Grand Prairie',\n",
       "       'Sherman', 'Kaufman', 'Tyler', 'Greenville', 'Henrietta',\n",
       "       'Farmersville', 'Ennis', 'Grapevine', 'Southlake', 'Rowlett',\n",
       "       'Point', 'Benbrook', 'Madill', 'Pottsboro', 'Haslet', 'Tishomingo',\n",
       "       'Allen', 'Bullard', 'Mineola', 'Waxahachie', 'Sunnyvale', 'Bryan',\n",
       "       'Lavon', 'Alvarado', 'Mexia', 'Joshua', 'Saginaw', 'Commerce',\n",
       "       'Van Alstyne', 'Flower Mound', 'Paris', 'EMORY', 'Ladonia',\n",
       "       'White Settlement', 'Royse City', 'Springtown', 'Whitewright',\n",
       "       'Red Oak', 'fort worth', 'Scroggins', 'Watauga', 'Pottstown',\n",
       "       'Saint Jo', 'Wylie', 'Godley', 'Bowie', 'Mansfield', 'Aledo',\n",
       "       'Cleburne', 'Cumby', 'ROWLETT', 'Queen Creek', 'Mingus',\n",
       "       'Richland Hills', 'Gun Barrel City', 'Wills Point', 'Celina',\n",
       "       'Denton', 'The Woodlands', 'Sanger', 'Mabank', 'Tulsa',\n",
       "       'Cockrell Hill', 'Sachse', 'Los Angeles', 'Lake Dallas',\n",
       "       'Cross Roads', 'Seagoville', 'Parker', 'Durant', 'Paradise',\n",
       "       'Tucson', 'Hawkins', 'Pantego', 'Anna', 'Nocona', 'Covington',\n",
       "       'Venus', 'WAXAHACHIE', 'The Colony', 'Argyle', 'River oaks',\n",
       "       'Bonham', 'Piney Flats', 'Ferris', 'Amherst', 'Terrell',\n",
       "       'Lone Oak', 'Tallulah', 'Humble', 'Winnsboro', 'Grandview',\n",
       "       'Hillsboro', 'Midlothian', 'FRISCO', 'Rocky Point', 'Greeneville ',\n",
       "       'Waco', 'Clifton', 'Londonderry', 'Pflugerville', 'Cooper',\n",
       "       'Highland Village', 'Pembroke Pines', 'Bay Shore', 'Merit',\n",
       "       'Bridgeport', 'Crowley', 'Tampa', 'Alba', 'Frankston', 'ALLEN',\n",
       "       'Everman', 'Haltom City', 'Burleson', 'Coppell', 'Boyd'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shelters['City'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
